<!DOCTYPE html>
<html>
	<head>
    <meta charset='utf-8' />
    <link rel="stylesheet" href="/css/styles.css?1436966512">
    <link rel="stylesheet" href="//cdn.bootcdn.net/ajax/libs/font-awesome/4.3.0/css/font-awesome.min.css">
    <link href='./images/favicon.png' rel='shortcut icon' />
    <meta content='width=device-width, minimum-scale=1.0, maximum-scale=1.0' name='viewport' />
    <title>REDIS sentinel-old -- Redis中国用户组（CRUG）</title>
    <meta name="description" content="redis
">
		<script src='./js/jquery-2.0.3.min.js?1426205838'></script>
		<script src='./js/slideout.js?1426205838'></script>
		<script src='./js/app.js?1436878127'></script>
		<script src='./js/base.js?1436878127'></script>
  </head>

<body class=''>
    <div class='mobile-menu slideout-menu'>
      <header class='menu-header'></header>
      <section class='menu-section'>
        <ul class='menu-section-list'>
          <li>
            <a class='home' src='/'>首页</a>
          </li>
          <li>
            <a href='./commands.html'>命令</a>
          </li>
          <li>
            <a href='./clients.html'>客户端</a>
          </li>
          <li>
            <a href='./documentation.html'>文档</a>
          </li>
          <li>
            <a href='./community.html'>社区</a>
          </li>
          <li>
            <a href='./download.html'>下载</a>
          </li>
          <li>
            <a href='./support.html'>支持</a>
          </li>
          <li>
            <a href='./topics/license.html'>许可</a>
          </li>
          <li>
            <a href='./update.html'>更新日志</a>
          </li>
          <li>
            <a href='./articles.html'>文章大全</a>
          </li>
		  <li>
            <a href='http://bbs.redis.cn' target='_blank'>论坛</a>
          </li>
          
        </ul>
      </section>
    </div>
    <div class='site-wrapper'>
      <header class='site-header'>
        <nav class='container'>
          <div class='mobile-header'>
            <button class='btn-hamburger js-slideout-toggle'>
              <span class='fa fa-bars'></span>
            </button>
            <a class='home' src='/'>
              <img alt='Redis' src='./images/redis-white.png' />
            </a>
          </div>
          <div class='desktop-header'>
            <a class='home' src='/'>
              <img alt='Redis' src='./images/redis-white.png' />
            </a>
            <a href='./commands.html'>命令</a>
            <a href='./clients.html'>客户端</a>
            <a href='./documentation.html'>文档</a>
            <a href='./community.html'>社区</a>
            <a href='./download.html'>下载</a>
            <a href='./support.html'>支持</a>
            <a href='./topics/license.html'>许可</a>
            <a href='./update.html'>更新日志</a>
            <a href='./articles.html'>文章大全</a>
			<a href='http://bbs.redis.cn' target='_blank'>论坛</a>
          </div>
        </nav>
      </header>
      <header class='site-header' style="background-color: #ffffff;">
        <!--
        <nav class='container'>
        	<a href="https://activity.huaweicloud.com/support_plan/index.html?utm_source=huawei&utm_medium=banner&utm_campaign=armredis&utm_content=0624&utm_term=crug" target="_blank">
				<img src="./images/bn/huawei_redis_08.png" style="width:100%;"/>
			</a>
        </nav>
      -->
      </header>
      <div class='site-content'>
<div class='text'>
	<article id='topic'>
		<h1 id="redis-sentinel-documentation">Redis Sentinel Documentation</h1>

<p>Redis Sentinel is a system designed to help managing Redis instances.
It performs the following three tasks:</p>

<ul>
  <li><strong>Monitoring</strong>. Sentinel constantly check if your master and slave instances are working as expected.</li>
  <li><strong>Notification</strong>. Sentinel can notify the system administrator, or another computer program, via an API, that something is wrong with one of the monitored Redis instances.</li>
  <li><strong>Automatic failover</strong>. If a master is not working as expected, Sentinel can start a failover process where a slave is promoted to master, the other additional slaves are reconfigured to use the new master, and the applications using the Redis server informed about the new address to use when connecting.</li>
</ul>

<p>Redis Sentinel is a distributed system, this means that usually you want to run
multiple Sentinel processes across your infrastructure, and this processes
will use agreement protocols in order to understand if a master is down and
to perform the failover.</p>

<p>Redis Sentinel is shipped as a stand-alone executable called <code class="language-plaintext highlighter-rouge">redis-sentinel</code>
but actually it is a special execution mode of the Redis server itself, and
can be also invoked using the <code class="language-plaintext highlighter-rouge">--sentinel</code> option of the normal <code class="language-plaintext highlighter-rouge">redis-sever</code>
executable.</p>

<p><strong>WARNING:</strong> Redis Sentinel is currently a work in progress. This document
describes how to use what we is already implemented, and may change as the
Sentinel implementation evolves.</p>

<p>Redis Sentinel is compatible with Redis 2.4.16 or greater, and redis 2.6.0-rc6 or greater.</p>

<h2 id="obtaining-sentinel">Obtaining Sentinel</h2>

<p>Currently Sentinel is part of the Redis <em>unstable</em> branch at GitHub.
To compile it you need to clone the <em>unstable</em> branch and compile Redis.
You’ll see a <code class="language-plaintext highlighter-rouge">redis-sentinel</code> executable in your <code class="language-plaintext highlighter-rouge">src</code> directory.</p>

<p>Alternatively you can use directly the <code class="language-plaintext highlighter-rouge">redis-server</code> executable itself,
starting it in Sentinel mode as specified in the next paragraph.</p>

<h2 id="running-sentinel">Running Sentinel</h2>

<p>If you are using the <code class="language-plaintext highlighter-rouge">redis-sentinel</code> executable (or if you have a symbolic
link with that name to the <code class="language-plaintext highlighter-rouge">redis-server</code> executable) you can run Sentinel
with the following command line:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>redis-sentinel /path/to/sentinel.conf
</code></pre></div></div>

<p>Otherwise you can use directly the <code class="language-plaintext highlighter-rouge">redis-server</code> executable starting it in
Sentinel mode:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>redis-server /path/to/sentinel.conf --sentinel
</code></pre></div></div>

<p>Both ways work the same.</p>

<h2 id="configuring-sentinel">Configuring Sentinel</h2>

<p>The Redis source distribution contains a file called <code class="language-plaintext highlighter-rouge">sentinel.conf</code>
that is a self-documented example configuration file you can use to
configure Sentinel, however a typical minimal configuration file looks like the
following:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sentinel monitor mymaster 127.0.0.1 6379 2
sentinel down-after-milliseconds mymaster 60000
sentinel failover-timeout mymaster 900000
sentinel can-failover mymaster yes
sentinel parallel-syncs mymaster 1

sentinel monitor resque 192.168.1.3 6380 4
sentinel down-after-milliseconds resque 10000
sentinel failover-timeout resque 900000
sentinel can-failover resque yes
sentinel parallel-syncs resque 5
</code></pre></div></div>

<p>The first line is used to tell Redis to monitor a master called <em>mymaster</em>,
that is at address 127.0.0.1 and port 6379, with a level of agreement needed
to detect this master as failing of 2 sentinels (if the agreement is not reached
the automatic failover does not start).</p>

<p>The other options are almost always in the form:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sentinel &lt;option_name&gt; &lt;master_name&gt; &lt;option_value&gt;
</code></pre></div></div>

<p>And are used for the following purposes:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">down-after-milliseconds</code> is the time in milliseconds an instance should not be reachable (either does not reply to our PINGs or it is replying with an error) for a Sentinel starting to think it is down. After this time has elapsed the Sentinel will mark an instance as <strong>subjectively down</strong> (also known as
<code class="language-plaintext highlighter-rouge">SDOWN</code>), that is not enough to
start the automatic failover. However if enough instances will think that there
is a subjectively down condition, then the instance is marked as
<strong>objectively down</strong>. The number of sentinels that needs to agree depends on
the configured agreement for this master.</li>
  <li><code class="language-plaintext highlighter-rouge">can-failover</code> tells this Sentinel if it should start a failover when an
instance is detected as objectively down (also called <code class="language-plaintext highlighter-rouge">ODOWN</code> for simplicity).
You may configure all the Sentinels to perform the failover if needed, or you
may have a few Sentinels used only to reach the agreement, and a few more
that are actually in charge to perform the failover.</li>
  <li><code class="language-plaintext highlighter-rouge">parallel-syncs</code> sets the number of slaves that can be reconfigured to use
the new master after a failover at the same time. The lower the number, the
more time it will take for the failover process to complete, however if the
slaves are configured to serve old data, you may not want all the slaves to
resync at the same time with the new master, as while the replication process
is mostly non blocking for a slave, there is a moment when it stops to load
the bulk data from the master during a resync. You may make sure only one
slave at a time is not reachable by setting this option to the value of 1.</li>
</ul>

<p>The other options are described in the rest of this document and
documented in the example sentinel.conf file shipped with the Redis
distribution.</p>

<h2 id="sdown-and-odown">SDOWN and ODOWN</h2>

<p>As already briefly mentioned in this document Redis Sentinel has two different
concepts of <em>being down</em>, one is called a <em>Subjectively Down</em> condition
(SDOWN) and is a down condition that is local to a given Sentinel instance.
Another is called <em>Objectively Down</em> condition (ODOWN) and is reached when
enough Sentinels (at least the number configured as the <code class="language-plaintext highlighter-rouge">quorum</code> parameter
of the monitored master) have an SDOWN condition, and get feedback from
other Sentinels using the <code class="language-plaintext highlighter-rouge">SENTINEL is-master-down-by-addr</code> command.</p>

<p>From the point of view of a Sentinel an SDOWN condition is reached if we
don’t receive a valid reply to PING requests for the number of seconds
specified in the configuration as <code class="language-plaintext highlighter-rouge">is-master-down-after-milliseconds</code>
parameter.</p>

<p>An acceptable reply to PING is one of the following:</p>

<ul>
  <li>PING replied with +PONG.</li>
  <li>PING replied with -LOADING error.</li>
  <li>PING replied with -MASTERDOWN error.</li>
</ul>

<p>Any other reply (or no reply) is considered non valid.</p>

<p>Note that SDOWN requires that no acceptable reply is received for the whole
interval configured, so for instance if the interval is 30000 milliseconds
(30 seconds) and we receive an acceptable ping reply every 29 seconds, the
instance is considered to be working.</p>

<p>The ODOWN condition <strong>only applies to masters</strong>. For other kind of instances
Sentinel don’t require any agreement, so the ODOWN state is never reached
for slaves and other sentinels.</p>

<p>The behavior of Redis Sentinel can be described by a set of rules that every
Sentinel follows. The complete behavior of Sentinel as a distributed system
composed of multiple Sentinels only results from this rules followed by
every single Sentinel instance. The following is the first set of rules.
In the course of this document more rules will be added in the appropriate
sections.</p>

<p><strong>Sentinel Rule #1</strong>: Every Sentinel sends a <strong>PING</strong> request to every known master, slave, and sentinel instance, every second.</p>

<p><strong>Sentinel Rule #2</strong>: An instance is Subjectively Down (<strong>SDOWN</strong>) if the latest valid reply to <strong>PING</strong> was received more than <code class="language-plaintext highlighter-rouge">down-after-milliseconds</code> milliseconds ago. Acceptable PING replies are: +PONG, -LOADING, -MASTERDOWN.</p>

<p><strong>Sentinel Rule #3</strong>: Every Sentinel is able to reply to the command <strong>SENTINEL is-master-down-by-addr <code class="language-plaintext highlighter-rouge">&lt;ip&gt; &lt;port&gt;</code></strong>. This command replies true if the specified address is the one of a master instance, and the master is in <strong>SDOWN</strong> state.</p>

<p><strong>Sentinel Rule #4</strong>: If a master is in <strong>SDOWN</strong> condition, every other Sentinel also monitoring this master, is queried for confirmation of this state, every second, using the <strong>SENTINEL is-master-down-by-addr</strong> command.</p>

<p><strong>Sentinel Rule #5</strong>: If a master is in <strong>SDOWN</strong> condition, and enough other Sentinels (to reach the configured quorum) agree about the condition, with a reply to <strong>SENTINEL is-master-down-by-addr</strong> that is no older than five seconds, then the master is marked as Objectively Down (<strong>ODOWN</strong>).</p>

<p><strong>Sentinel Rule #6</strong>: Every Sentinel sends an <strong>INFO</strong> request to every known master and slave instance, one time every 10 seconds. If a master is in <strong>ODOWN</strong> condition, its slaves are asked for <strong>INFO</strong> every second instead of being asked every 10 seconds.</p>

<p><strong>Sentinel Rule #7</strong>: If the <strong>first</strong> INFO reply a Sentinel receives about a master shows that it is actually a slave, Sentinel will update the configuration to actually monitor the master reported by the INFO output instead. So it is safe to start Sentinel against slaves.</p>

<h2 id="sentinels-and-slaves-auto-discovery">Sentinels and Slaves auto discovery</h2>

<p>While Sentinels stay connected with other Sentinels in order to reciprocally
check the availability of each other, and to exchange messages, you don’t
need to configure the other Sentinel addresses in every Sentinel instance you
run, as Sentinel uses the Redis master Pub/Sub capabilities in order to
discover the other Sentinels that are monitoring the same master.</p>

<p>This is obtained by sending <em>Hello Messages</em> into the channel named
<code class="language-plaintext highlighter-rouge">__sentinel__:hello</code>.</p>

<p>Similarly you don’t need to configure what is the list of the slaves attached
to a master, as Sentinel will auto discover this list querying Redis.</p>

<p><strong>Sentinel Rule #8</strong>: Every Sentinel publishes a message to every monitored master Pub/Sub channel <code class="language-plaintext highlighter-rouge">__sentinel__:hello</code>, every five seconds, announcing its presence with ip, port, runid, and ability to failover (accordingly to <code class="language-plaintext highlighter-rouge">can-failover</code> configuration directive in <code class="language-plaintext highlighter-rouge">sentinel.conf</code>).</p>

<p><strong>Sentinel Rule #9</strong>: Every Sentinel is subscribed to the Pub/Sub channel <code class="language-plaintext highlighter-rouge">__sentinel__:hello</code> of every master, looking for unknown sentinels. When new sentinels are detected, we add them as sentinels of this master.</p>

<p><strong>Sentinel Rule #10</strong>: Before adding a new sentinel to a master a Sentinel always checks if there is already a sentinel with the same runid or the same address (ip and port pair). In that case all the matching sentinels are removed, and the new added.</p>

<h1 id="sentinel-api">Sentinel API</h1>

<p>By default Sentinel runs using TCP port 26379 (note that 6379 is the normal
Redis port). Sentinels accept commands using the Redis protocol, so you can
use <code class="language-plaintext highlighter-rouge">redis-cli</code> or any other unmodified Redis client in order to talk with
Sentinel.</p>

<p>There are two ways to talk with Sentinel: it is possible to directly query
it to check what is the state of the monitored Redis instances from its point
of view, to see what other Sentinels it knows, and so forth.</p>

<p>An alternative is to use Pub/Sub to receive <em>push style</em> notifications from
Sentinels, every time some event happens, like a failover, or an instance
entering an error condition, and so forth.</p>

<h2 id="sentinel-commands">Sentinel commands</h2>

<p>The following is a list of accepted commands:</p>

<ul>
  <li><strong>PING</strong> this command simply returns PONG.</li>
  <li><strong>SENTINEL masters</strong> show a list of monitored masters and their state.</li>
  <li><strong>SENTINEL slaves <code class="language-plaintext highlighter-rouge">&lt;master name&gt;</code></strong> show a list of slaves for this master, and their state.</li>
  <li><strong>SENTINEL is-master-down-by-addr <code class="language-plaintext highlighter-rouge">&lt;ip&gt; &lt;port&gt;</code></strong> return a two elements multi bulk reply where the first is 0 or 1 (0 if the master with that address is known and is in <code class="language-plaintext highlighter-rouge">SDOWN</code> state, 1 otherwise). The second element of the reply is the
<em>subjective leader</em> for this master, that is, the <code class="language-plaintext highlighter-rouge">runid</code> of the Redis
Sentinel instance that should perform the failover accordingly to the queried
instance.</li>
  <li><strong>SENTINEL get-master-addr-by-name <code class="language-plaintext highlighter-rouge">&lt;master name&gt;</code></strong> return the ip and port number of the master with that name. If a failover is in progress or terminated successfully for this master it returns the address and port of the promoted slave.</li>
  <li><strong>SENTINEL reset <code class="language-plaintext highlighter-rouge">&lt;pattern&gt;</code></strong> this command will reset all the masters with matching name. The pattern argument is a glob-style pattern. The reset process clears any previous state in a master (including a failover in progress), and removes every slave and sentinel already discovered and associated with the master.</li>
</ul>

<h2 id="pubsub-messages">Pub/Sub Messages</h2>

<p>A client can use a Sentinel as it was a Redis compatible Pub/Sub server
(but you can’t use <code class="language-plaintext highlighter-rouge">PUBLISH</code>) in order to <code class="language-plaintext highlighter-rouge">SUBSCRIBE</code> or <code class="language-plaintext highlighter-rouge">PSUBSCRIBE</code> to
channels and get notified about specific events.</p>

<p>The channel name is the same as the name of the event. For instance the
channel named <code class="language-plaintext highlighter-rouge">+sdown</code> will receive all the notifications related to instances
entering an <code class="language-plaintext highlighter-rouge">SDOWN</code> condition.</p>

<p>To get all the messages simply subscribe using <code class="language-plaintext highlighter-rouge">PSUBSCRIBE *</code>.</p>

<p>The following is a list of channels and message formats you can receive using
this API. The first word is the channel / event name, the rest is the format of the data.</p>

<p>Note: where <em>instance details</em> is specified it means that the following arguments are provided to identify the target instance:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;instance-type&gt; &lt;name&gt; &lt;ip&gt; &lt;port&gt; @ &lt;master-name&gt; &lt;master-ip&gt; &lt;master-port&gt;
</code></pre></div></div>

<p>The part identifying the master (from the @ argument to the end) is optional
and is only specified if the instance is not a master itself.</p>

<ul>
  <li><strong>+reset-master</strong> <code class="language-plaintext highlighter-rouge">&lt;instance details&gt;</code> – The master was reset.</li>
  <li><strong>+slave</strong> <code class="language-plaintext highlighter-rouge">&lt;instance details&gt;</code> – A new slave was detected and attached.</li>
  <li><strong>+failover-state-reconf-slaves</strong> <code class="language-plaintext highlighter-rouge">&lt;instance details&gt;</code> – Failover state changed to <code class="language-plaintext highlighter-rouge">reconf-slaves</code> state.</li>
  <li><strong>+failover-detected</strong> <code class="language-plaintext highlighter-rouge">&lt;instance details&gt;</code> – A failover started by another Sentinel or any other external entity was detected (An attached slave turned into a master).</li>
  <li><strong>+slave-reconf-sent</strong> <code class="language-plaintext highlighter-rouge">&lt;instance details&gt;</code> – The leader sentinel sent the <code class="language-plaintext highlighter-rouge">SLAVEOF</code> command to this instance in order to reconfigure it for the new slave.</li>
  <li><strong>+slave-reconf-inprog</strong> <code class="language-plaintext highlighter-rouge">&lt;instance details&gt;</code> – The slave being reconfigured showed to be a slave of the new master ip:port pair, but the synchronization process is not yet complete.</li>
  <li><strong>+slave-reconf-done</strong> <code class="language-plaintext highlighter-rouge">&lt;instance details&gt;</code> – The slave is now synchronized with the new master.</li>
  <li><strong>-dup-sentinel</strong> <code class="language-plaintext highlighter-rouge">&lt;instance details&gt;</code> – One or more sentinels for the specified master were removed as duplicated (this happens for instance when a Sentinel instance is restarted).</li>
  <li><strong>+sentinel</strong> <code class="language-plaintext highlighter-rouge">&lt;instance details&gt;</code> – A new sentinel for this master was detected and attached.</li>
  <li><strong>+sdown</strong> <code class="language-plaintext highlighter-rouge">&lt;instance details&gt;</code> – The specified instance is now in Subjectively Down state.</li>
  <li><strong>-sdown</strong> <code class="language-plaintext highlighter-rouge">&lt;instance details&gt;</code> – The specified instance is no longer in Subjectively Down state.</li>
  <li><strong>+odown</strong> <code class="language-plaintext highlighter-rouge">&lt;instance details&gt;</code> – The specified instance is now in Objectively Down state.</li>
  <li><strong>-odown</strong> <code class="language-plaintext highlighter-rouge">&lt;instance details&gt;</code> – The specified instance is no longer in Objectively Down state.</li>
  <li><strong>+failover-takedown</strong> <code class="language-plaintext highlighter-rouge">&lt;instance details&gt;</code> – 25% of the configured failover timeout has elapsed, but this sentinel can’t see any progress, and is the new leader. It starts to act as the new leader reconfiguring the remaining slaves to replicate with the new master.</li>
  <li><strong>+failover-triggered</strong> <code class="language-plaintext highlighter-rouge">&lt;instance details&gt;</code> – We are starting a new failover as a the leader sentinel.</li>
  <li><strong>+failover-state-wait-start</strong> <code class="language-plaintext highlighter-rouge">&lt;instance details&gt;</code> – New failover state is <code class="language-plaintext highlighter-rouge">wait-start</code>: we are waiting a fixed number of seconds, plus a random number of seconds before starting the failover.</li>
  <li><strong>+failover-state-select-slave</strong> <code class="language-plaintext highlighter-rouge">&lt;instance details&gt;</code> – New failover state is <code class="language-plaintext highlighter-rouge">select-slave</code>: we are trying to find a suitable slave for promotion.</li>
  <li><strong>no-good-slave</strong> <code class="language-plaintext highlighter-rouge">&lt;instance details&gt;</code> – There is no good slave to promote. Currently we’ll try after some time, but probably this will change and the state machine will abort the failover at all in this case.</li>
  <li><strong>selected-slave</strong> <code class="language-plaintext highlighter-rouge">&lt;instance details&gt;</code> – We found the specified good slave to promote.</li>
  <li><strong>failover-state-send-slaveof-noone</strong> <code class="language-plaintext highlighter-rouge">&lt;instance details&gt;</code> – We are trying to reconfigure the promoted slave as master, waiting for it to switch.</li>
  <li><strong>failover-end-for-timeout</strong> <code class="language-plaintext highlighter-rouge">&lt;instance details&gt;</code> – The failover terminated for timeout. If we are the failover leader, we sent a <em>best effort</em> <code class="language-plaintext highlighter-rouge">SLAVEOF</code> command to all the slaves yet to reconfigure.</li>
  <li><strong>failover-end</strong> <code class="language-plaintext highlighter-rouge">&lt;instance details&gt;</code> – The failover terminated with success. All the slaves appears to be reconfigured to replicate with the new master.</li>
  <li><strong>switch-master</strong> <code class="language-plaintext highlighter-rouge">&lt;master name&gt; &lt;oldip&gt; &lt;oldport&gt; &lt;newip&gt; &lt;newport&gt;</code> – We are starting to monitor the new master, using the same name of the old one. The old master will be completely removed from our tables.</li>
  <li><strong>failover-abort-x-sdown</strong> <code class="language-plaintext highlighter-rouge">&lt;instance details&gt;</code> – The failover was undone (aborted) because the promoted slave appears to be in extended SDOWN state.</li>
  <li><strong>-slave-reconf-undo</strong> <code class="language-plaintext highlighter-rouge">&lt;instance details&gt;</code> – The failover aborted so we sent a <code class="language-plaintext highlighter-rouge">SLAVEOF</code> command to the specified instance to reconfigure it back to the original master instance.</li>
  <li><strong>+tilt</strong> – Tilt mode entered.</li>
  <li><strong>-tilt</strong> – Tilt mode exited.</li>
</ul>

<h1 id="sentinel-failover">Sentinel failover</h1>

<p>The failover process consists on the following steps:</p>

<ul>
  <li>Recognize that the master is in ODOWN state.</li>
  <li>Understand who is the Sentinel that should start the failover, called <strong>The Leader</strong>. All the other Sentinels will be <strong>The Observers</strong>.</li>
  <li>The leader selects a slave to promote to master.</li>
  <li>The promoted slave is turned into a master with the command <strong>SLAVEOF NO ONE</strong>.</li>
  <li>The observers see that a slave was turned into a master, so they know the failover started. <strong>Note:</strong> this means that any event that turns one of the slaves of a monitored master into a master (<code class="language-plaintext highlighter-rouge">SLAVEOF NO ONE</code> command) will be sensed as the start of a failover process.</li>
  <li>All the other slaves attached to the original master are configured with the <strong>SLAVEOF</strong> command in order to start the replication process with the new master.</li>
  <li>The leader terminates the failover process when all the slaves are reconfigured. It removes the old master from the table of monitored masters and adds the new master, <em>under the same name</em> of the original master.</li>
  <li>The observers detect the end of the failover process when all the slaves are reconfigured. They remove the old master from the table and start monitoring the new master, exactly as the leader does.</li>
</ul>

<p>The election of the Leader is performed using the same mechanism used to reach
the ODOWN state, that is, the <strong>SENTINEL is-master-down-by-addr</strong> command.
It returns the leader from the point of view of the queried Sentinel, we call
it the <strong>Subjective Leader</strong>, and is selected using the following rule:</p>

<ul>
  <li>We remove all the Sentinels that can’t failover for configuration (this information is propagated using the Hello Channel to all the Sentinels).</li>
  <li>We remove all the Sentinels in SDOWN, disconnected, or with the last ping reply received more than <code class="language-plaintext highlighter-rouge">SENTINEL_INFO_VALIDITY_TIME</code> milliseconds ago (currently defined as 5 seconds).</li>
  <li>Of all the remaining instances, we get the one with the lowest <code class="language-plaintext highlighter-rouge">runid</code>, lexicographically (every Redis instance has a Run ID, that is an identifier of every single execution).</li>
</ul>

<p>For a Sentinel to sense to be the <strong>Objective Leader</strong>, that is, the Sentinel that should start the failover process, the following conditions are needed.</p>

<ul>
  <li>It thinks it is the subjective leader itself.</li>
  <li>It receives acknowledges from other Sentinels about the fact it is the leader: at least 50% plus one of all the Sentinels that were able to reply to the <code class="language-plaintext highlighter-rouge">SENTINEL is-master-down-by-addr</code> request should agree it is the leader, and additionally we need a total level of agreement at least equal to the configured quorum of the master instance that we are going to failover.</li>
</ul>

<p>Once a Sentinel things it is the Leader, the failover starts, but there is always a delay of five seconds plus an additional random delay. This is an additional layer of protection because if during this period we see another instance turning a slave into a master, we detect it as another instance staring the failover and turn ourselves into an observer instead. This is just a redundancy layer and should in theory never happen.</p>

<p><strong>Sentinel Rule #11</strong>: A <strong>Good Slave</strong> is a slave with the following requirements:</p>
<ul>
  <li>It is not in SDOWN nor in ODOWN condition.</li>
  <li>We have a valid connection to it currently (not in DISCONNECTED state).</li>
  <li>Latest PING reply we received from it is not older than five seconds.</li>
  <li>Latest INFO reply we received from it is not older than five seconds.</li>
  <li>The latest INFO reply reported that the link with the master is down for no more than the time elapsed since we saw the master entering SDOWN state, plus ten times the configured <code class="language-plaintext highlighter-rouge">down_after_milliseconds</code> parameter. So for instance if a Sentinel is configured to sense the SDOWN condition after 10 seconds, and the master is down since 50 seconds, we accept a slave as a Good Slave only if the replication link was disconnected less than <code class="language-plaintext highlighter-rouge">50+(10*10)</code> seconds (two minutes and half more or less).</li>
  <li>It is not flagged as DEMOTE (see the section about resurrecting masters).</li>
</ul>

<p><strong>Sentinel Rule #12</strong>: A <strong>Subjective Leader</strong> from the point of view of a Sentinel, is the Sentinel (including itself) with the lower runid monitoring a given master, that also replied to PING less than 5 seconds ago, reported to be able to do the failover via Pub/Sub hello channel, and is not in DISCONNECTED state.</p>

<p><strong>Sentinel Rule #12</strong>: If a master is down we ask <code class="language-plaintext highlighter-rouge">SENTINEL is-master-down-by-addr</code> to every other connected Sentinel as explained in Sentinel Rule #4. This command will also reply with the runid of the <strong>Subjective Leader</strong> from the point of view of the asked Sentinel. A given Sentinel believes to be the <strong>Objective Leader</strong> of a master if it is reported to be the subjective leader by N Sentinels (including itself), where:</p>
<ul>
  <li>N must be equal or greater to the configured quorum for this master.</li>
  <li>N mast be equal or greater to the majority of the voters (<code class="language-plaintext highlighter-rouge">num_votres/2+1</code>), considering only the Sentinels that also reported the master to be down.</li>
</ul>

<p><strong>Sentinel Rule #13</strong>: A Sentinel starts the failover as a <strong>Leader</strong> (that is, the Sentinel actually sending the commands to reconfigure the Redis servers) if the following conditions are true at the same time:</p>
<ul>
  <li>The master is in ODOWN condition.</li>
  <li>The Sentinel is configured to perform the failover with <code class="language-plaintext highlighter-rouge">can-failover</code> set to yes.</li>
  <li>There is at least a Good Slave from the point of view of the Sentinel.</li>
  <li>The Sentinel believes to be the Objective Leader.</li>
  <li>There is no failover in progress already detected for this master.</li>
</ul>

<p><strong>Sentinel Rule #14</strong>: A Sentinel detects a failover as an <strong>Observer</strong> (that is, the Sentinel just follows the failover generating the appropriate events in the log file and Pub/Sub interface, but without actively reconfiguring instances) if the following conditions are true at the same time:</p>
<ul>
  <li>There is no failover already in progress.</li>
  <li>A slave instance of the monitored master turned into a master.
However the failover <strong>will NOT be sensed as started if the slave instance turns into a master and at the same time the runid has changed</strong> from the previous one. This means the instance turned into a master because of a restart, and is not a valid condition to consider it a slave election.</li>
</ul>

<p><strong>Sentinel Rule #15</strong>: A Sentinel starting a failover as leader does not immediately starts it. It enters a state called <strong>wait-start</strong>, that lasts a random amount of time between 5 seconds and 15 seconds. During this time <strong>Sentinel Rule #14</strong> still applies: if a valid slave promotion is detected the failover as leader is aborted and the failover as observer is detected.</p>

<h2 id="end-of-failover">End of failover</h2>

<p>The failover process is considered terminated from the point of view of a
single Sentinel if:</p>

<ul>
  <li>The promoted slave is not in SDOWN condition.</li>
  <li>A slave was promoted as new master.</li>
  <li>All the other slaves are configured to use the new master.</li>
</ul>

<p>Note: Slaves that are in SDOWN state are ignored.</p>

<p>Also the failover state is considered terminate if:</p>

<ul>
  <li>The promoted slave is not in SDOWN condition.</li>
  <li>A slave was promoted as new master.</li>
  <li>At least <code class="language-plaintext highlighter-rouge">failover-timeout</code> milliseconds elapsed since the last progress.</li>
</ul>

<p>The <code class="language-plaintext highlighter-rouge">failover-timeout</code> value can be configured in sentinel.conf for every
different slave.</p>

<p>Note that when a leader terminates a failover for timeout, it sends a
<code class="language-plaintext highlighter-rouge">SLAVEOF</code> command in a best-effort way to all the slaves yet to be
configured, in the hope that they’ll receive the command and replicate
with the new master eventually.</p>

<p><strong>Sentinel Rule #16</strong> A failover is considered complete if for a leader or observer if:</p>
<ul>
  <li>One slave was promoted to master (and the Sentinel can detect that this actually happened via INFO output), and all the additional slaves are all configured to replicate with the new slave (again, the sentinel needs to sense it using the INFO output).</li>
  <li>There is already a correctly promoted slave, but the configured <code class="language-plaintext highlighter-rouge">failover-timeout</code> time has already elapsed without any progress in the reconfiguration of the additional slaves. In this case a leader sends a best effort <code class="language-plaintext highlighter-rouge">SLAVEOF</code> command is sent to all the not yet configured slaves.
In both the two above conditions the promoted slave <strong>must be reachable</strong> (not in SDOWN state), otherwise a failover is never considered to be complete.</li>
</ul>

<h2 id="leader-failing-during-failover">Leader failing during failover</h2>

<p>If the leader fails when it has yet to promote the slave into a master, and it
fails in a way that makes it in SDOWN state from the point of view of the other
Sentinels, if enough Sentinels remained to reach the quorum the failover
will automatically continue using a new leader (the subjective leader of
all the remaining Sentinels will change because of the SDOWN state of the
previous leader).</p>

<p>If the failover was already in progress and the slave
was already promoted, and possibly a few other slaves were already reconfigured,
an observer that is the new objective leader will continue the failover in
case no progresses are made for more than 25% of the time specified by the
<code class="language-plaintext highlighter-rouge">failover-timeout</code> configuration option.</p>

<p>Note that this is safe as multiple Sentinels trying to reconfigure slaves
with duplicated SLAVEOF commands do not create any race condition, but at the
same time we want to be sure that all the slaves are reconfigured in the
case the original leader is no longer working.</p>

<p><strong>Sentinel Rule #17</strong> A Sentinel that is an observer for a failover in progress
will turn itself into a failover leader, continuing the configuration of the
additional slaves, if all the following conditions are true:</p>
<ul>
  <li>A failover is in progress, and this Sentinel is an observer.</li>
  <li>It detects to be an objective leader (so likely the previous leader is no longer reachable by other sentinels).</li>
  <li>At least 25% of the configured <code class="language-plaintext highlighter-rouge">failover-timeout</code> has elapsed without any progress in the observed failover process.</li>
</ul>

<h2 id="promoted-slave-failing-during-failover">Promoted slave failing during failover</h2>

<p>If the promoted slave has an active SDOWN condition, a Sentinel will never
sense the failover as terminated.</p>

<p>Additionally if there is an <em>extended SDOWN condition</em> (that is an SDOWN that
lasts for more than ten times <code class="language-plaintext highlighter-rouge">down-after-milliseconds</code> milliseconds) the
failover is aborted (this happens for leaders and observers), and the master
starts to be monitored again as usually, so that a new failover can start with
a different slave in case the master is still failing.</p>

<p>Note that when this happens it is possible that there are a few slaves already
configured to replicate from the (now failing) promoted slave, so when the
leader sentinel aborts a failover it sends a <code class="language-plaintext highlighter-rouge">SLAVEOF</code> command to all the
slaves already reconfigured or in the process of being reconfigured to switch
the configuration back to the original master.</p>

<p><strong>Sentinel Rule #18</strong> A Sentinel will consider the failover process aborted, both when acting as leader and when acting as an observer, in the following conditions are true:</p>
<ul>
  <li>A failover is in progress and a slave to promote was already selected (or in the case of the observer was already detected as master).</li>
  <li>The promoted slave is in <strong>Extended SDOWN</strong> condition (continually in SDOWN condition for at least ten times the configured <code class="language-plaintext highlighter-rouge">down-after-milliseconds</code>).</li>
</ul>

<h2 id="resurrecting-master">Resurrecting master</h2>

<p>After the failover, at some point the old master may return back online. Starting with Redis 2.6.13 Sentinel is able to handle this condition by automatically reconfiguring the old master as a slave of the new master.</p>

<p>This happens in the following way:</p>

<ul>
  <li>After the failover has started from the point of view of a Sentinel, either as a leader, or as an observer that detected the promotion of a slave, the old master is put in the list of slaves of the new master, but with a special <code class="language-plaintext highlighter-rouge">DEMOTE</code> flag (the flag can be seen in the <code class="language-plaintext highlighter-rouge">SENTINEL SLAVES</code> command output).</li>
  <li>Once the master is back online and it is possible to contact it again, if it still claims to be a master (from INFO output) Sentinels will send a <code class="language-plaintext highlighter-rouge">SLAVEOF</code> command trying to reconfigure it. Once the instance claims to be a slave, the <code class="language-plaintext highlighter-rouge">DEMOTE</code> flag is cleared.</li>
</ul>

<p>There is no single Sentinel in charge of turning the old master into a slave, so the process is resistant against failing sentinels. At the same time instances with the <code class="language-plaintext highlighter-rouge">DEMOTE</code> flag set are never selected as promotable slaves.</p>

<p>In this specific case the <code class="language-plaintext highlighter-rouge">+slave</code> event is only generated only when the old master will report to be actually a slave again in its <code class="language-plaintext highlighter-rouge">INFO</code> output.</p>

<p><strong>Sentinel Rule #19</strong>: Once the failover starts (either as observer or leader), the old master is added as a slave of the new master, flagged as <code class="language-plaintext highlighter-rouge">DEMOTE</code>.</p>

<p><strong>Sentinel Rule #20</strong>: A slave instance claiming to be a master, and flagged as <code class="language-plaintext highlighter-rouge">DEMOTE</code>, is reconfigured via <code class="language-plaintext highlighter-rouge">SLAVEOF</code> every time a Sentinel receives an <code class="language-plaintext highlighter-rouge">INFO</code> output where the wrong role is detected.</p>

<p><strong>Sentinel Rule #21</strong>: The <code class="language-plaintext highlighter-rouge">DEMOTE</code> flag is cleared as soon as an <code class="language-plaintext highlighter-rouge">INFO</code> output shows the instance to report itself as a slave.</p>

<h2 id="manual-interactions">Manual interactions</h2>

<ul>
  <li>TODO: Manually triggering a failover with SENTINEL FAILOVER.</li>
  <li>TODO: Pausing Sentinels with SENTINEL PAUSE, RESUME.</li>
</ul>

<h2 id="the-failback-process">The failback process</h2>

<ul>
  <li>TODO: Sentinel does not perform automatic Failback.</li>
  <li>TODO: Document correct steps for the failback.</li>
</ul>

<h2 id="clients-configuration-update">Clients configuration update</h2>

<p>Work in progress.</p>

<h2 id="tilt-mode">TILT mode</h2>

<p>Redis Sentinel is heavily dependent on the computer time: for instance in
order to understand if an instance is available it remembers the time of the
latest successful reply to the PING command, and compares it with the current
time to understand how old it is.</p>

<p>However if the computer time changes in an unexpected way, or if the computer
is very busy, or the process blocked for some reason, Sentinel may start to
behave in an unexpected way.</p>

<p>The TILT mode is a special “protection” mode that a Sentinel can enter when
something odd is detected that can lower the reliability of the system.
The Sentinel timer interrupt is normally called 10 times per second, so we
expect that more or less 100 milliseconds will elapse between two calls
to the timer interrupt.</p>

<p>What a Sentinel does is to register the previous time the timer interrupt
was called, and compare it with the current call: if the time difference
is negative or unexpectedly big (2 seconds or more) the TILT mode is entered
(or if it was already entered the exit from the TILT mode postponed).</p>

<p>When in TILT mode the Sentinel will continue to monitor everything, but:</p>

<ul>
  <li>It stops acting at all.</li>
  <li>It starts to reply negatively to <code class="language-plaintext highlighter-rouge">SENTINEL is-master-down-by-addr</code> requests as the ability to detect a failure is no longer trusted.</li>
</ul>

<p>If everything appears to be normal for 30 second, the TILT mode is exited.</p>

<h2 id="handling-of--busy-state">Handling of -BUSY state</h2>

<p>(Warning: Yet not implemented)</p>

<p>The -BUSY error is returned when a script is running for more time than the
configured script time limit. When this happens before triggering a fail over
Redis Sentinel will try to send a “SCRIPT KILL” command, that will only
succeed if the script was read-only.</p>

<h2 id="notifications-via-user-script">Notifications via user script</h2>

<p>Work in progress.</p>

<h2 id="suggested-setup">Suggested setup</h2>

<p>Work in progress.</p>

<h1 id="appendix-a---implementation-and-algorithms">APPENDIX A - Implementation and algorithms</h1>

<h2 id="duplicate-sentinels-removal">Duplicate Sentinels removal</h2>

<p>In order to reach the configured quorum we absolutely want to make sure that
the quorum is reached by different physical Sentinel instances. Under
no circumstance we should get agreement from the same instance that for some
reason appears to be two or multiple distinct Sentinel instances.</p>

<p>This is enforced by an aggressive removal of duplicated Sentinels: every time
a Sentinel sends a message in the Hello Pub/Sub channel with its address
and runid, if we can’t find a perfect match (same runid and address) inside
the Sentinels table for that master, we remove any other Sentinel with the same
runid OR the same address. And later add the new Sentinel.</p>

<p>For instance if a Sentinel instance is restarted, the Run ID will be different,
and the old Sentinel with the same IP address and port pair will be removed.</p>

<h2 id="selection-of-the-slave-to-promote">Selection of the Slave to promote</h2>

<p>If a master has multiple slaves, the slave to promote to master is selected
checking the slave priority (a new configuration option of Redis instances
that is propagated via INFO output, still not implemented), and picking the
one with lower priority value (it is an integer similar to the one of the
MX field of the DNS system).</p>

<p>All the slaves that appears to be disconnected from the master for a long
time are discarded.</p>

<p>If slaves with the same priority exist, the one with the lexicographically
smaller Run ID is selected.</p>

<p>Note: because currently slave priority is not implemented, the selection is
performed only discarding unreachable slaves and picking the one with the
lower Run ID.</p>

<p><strong>Sentinel Rule #22</strong>: A Sentinel performing the failover as leader will select the slave to promote, among the existing <strong>Good Slaves</strong> (See rule #11), taking the one with the lower slave priority. When priority is the same the slave with lexicographically lower runid is preferred.</p>

<h1 id="appendix-b---get-started-with-sentinel-in-five-minutes">APPENDIX B - Get started with Sentinel in five minutes</h1>

<p>If you want to try Redis Sentinel, please follow this steps:</p>

<ul>
  <li>Clone the <em>unstable</em> branch of the Redis repository at github (it is the default branch).</li>
  <li>Compile it with “make”.</li>
  <li>Start a few normal Redis instances, using the <code class="language-plaintext highlighter-rouge">redis-server</code> compiled in the <em>unstable</em> branch. One master and one slave is enough.</li>
  <li>Use the <code class="language-plaintext highlighter-rouge">redis-sentinel</code> executable to start three instances of Sentinel, with <code class="language-plaintext highlighter-rouge">redis-sentinel /path/to/config</code>.</li>
</ul>

<p>To create the three configurations just create three files where you put something like that:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>port 26379
sentinel monitor mymaster 127.0.0.1 6379 2
sentinel down-after-milliseconds mymaster 5000
sentinel failover-timeout mymaster 900000
sentinel can-failover mymaster yes
sentinel parallel-syncs mymaster 1
</code></pre></div></div>

<p>Note: where you see <code class="language-plaintext highlighter-rouge">port 26379</code>, use 26380 for the second Sentinel, and 26381 for the third Sentinel (any other different non colliding port will do of course). Also note that the <code class="language-plaintext highlighter-rouge">down-after-milliseconds</code> configuration option is set to just five seconds, that is a good value to play with Sentinel, but not good for production environments.</p>

<p>At this point you should see something like the following in every Sentinel you are running:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[4747] 23 Jul 14:49:15.883 * +slave slave 127.0.0.1:6380 127.0.0.1 6380 @ mymaster 127.0.0.1 6379
[4747] 23 Jul 14:49:19.645 * +sentinel sentinel 127.0.0.1:26379 127.0.0.1 26379 @ mymaster 127.0.0.1 6379
[4747] 23 Jul 14:49:21.659 * +sentinel sentinel 127.0.0.1:26381 127.0.0.1 26381 @ mymaster 127.0.0.1 6379

redis-cli -p 26379 sentinel masters
1)  1) "name"
    2) "mymaster"
    3) "ip"
    4) "127.0.0.1"
    5) "port"
    6) "6379"
    7) "runid"
    8) "66215809eede5c0fdd20680cfb3dbd3bdf70a6f8"
    9) "flags"
   10) "master"
   11) "pending-commands"
   12) "0"
   13) "last-ok-ping-reply"
   14) "515"
   15) "last-ping-reply"
   16) "515"
   17) "info-refresh"
   18) "5116"
   19) "num-slaves"
   20) "1"
   21) "num-other-sentinels"
   22) "2"
   23) "quorum"
   24) "2"
</code></pre></div></div>

<p>To see how the failover works, just put down your slave (for instance sending <code class="language-plaintext highlighter-rouge">DEBUG SEGFAULT</code> to crash it) and see what happens.</p>

<p>This HOWTO is a work in progress, more information will be added in the near future.</p>

	</article>
	
</div>
<script>
		if(isRediscnPc()){
			var s = "_" + Math.random().toString(36).slice(2);
			document.write('<div style="margin-bottom:10px;" id="' + s + '"></div>');
			(window.slotbydup = window.slotbydup || []).push({
				id: "u3556359",
				container:  s
			});
			document.write('<scr'+'ipt type="text/javascr'+'ipt" src="//cpro.baidustatic.com/cpro/ui/c.js" async="async" defer="defer" ></scr'+'ipt>');
		}
	
</script>
<footer class='site-footer'>
        <div class='container'>
          本站资源翻译自<a href="http://redis.io" target="_blank">redis.io</a>，
					由<a src="./aboutus.html">redis.cn翻译团队</a>翻译，
					更新日志请点击<a src="./update.html">这里</a>查看，
					翻译原文版权归redis.io官方所有，翻译不正确的地方欢迎大家指出。<br> 
					感谢各界爱心人士的热心捐赠，CRUG的成长离不开大家的帮助和支持，特别是<a src="./donation.html">Redis捐赠清单</a>里面的各位伙伴。<br>
					联系Email:<a href="mailto:admin@redis.cn">admin@redis.cn</a>，
					redis交流群：<a href="#">579708237</a> &nbsp; 
					<a href="https://beian.miit.gov.cn" target="_blank">京ICP备15003959号-2</a> &nbsp;
					<br>    
					<span style="font-weight:bold;color:#000000;">友情链接：</span>
					<a href="https://promotion.aliyun.com/ntms/yunparter/invite.html?userCode=fvilu0rm" target="_blank">阿里云</a> &nbsp;
					<a href="http://mdba.cn" target="_blank">DBA的罗浮宫</a> &nbsp;
					<a href="http://mdba.cn" target="_blank">VIP-陈群博客</a> &nbsp;
					<a href="http://lib.csdn.net/base/redis" target="_blank">Redis-知识库</a> &nbsp;
					<a href="http://www.kubernetes.org.cn" target="_blank" >Kubernetes</a> &nbsp;	
					<a href="https://www.fanghouguo.com" target="_blank" >方后国的博客</a> &nbsp;	
					<a href="https://aff.gae1s.com/aff.php?aff=10022" target="_blank" >ChromeGAE</a> &nbsp;	
					<a href="http://top.chinaz.com/" target="_blank" >网站排行榜</a> &nbsp;	
        </div>
      </footer>
    </div>
  </body>
</html>


<script>
	if(!isMobileBrowser()){
		window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"16"},"slide":{"type":"slide","bdImg":"0","bdPos":"right","bdTop":"54.5"},"image":{"viewList":["qzone","tsina","tqq","renren","weixin"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["qzone","tsina","tqq","renren","weixin"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='//bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
	}
</script>

<script type='text/javascript'>
lastScrollY=0;

function heartBeat(){ 
	var diffY;
	if (document.documentElement && document.documentElement.scrollTop)
	diffY = document.documentElement.scrollTop;
	else if (document.body)
	diffY = document.body.scrollTop
	else
	{/*Netscape stuff*/}
	//alert(diffY);
	percent=.1*(diffY-lastScrollY); 
	if(percent>0)percent=Math.ceil(percent); 
	else percent=Math.floor(percent); 
	document.getElementById("lovexin12").style.top=parseInt(document.getElementById
	("lovexin12").style.top)+percent+"px";
	document.getElementById("lovexin14").style.top=parseInt(document.getElementById
	("lovexin12").style.top)+percent+"px";
	lastScrollY=lastScrollY+percent; 
	//alert(lastScrollY);
}

if(!isMobileBrowser()){
		//suspendcode12="<DIV id=\"lovexin12\" style='width:120px;height:270px;left:2px;POSITION:absolute;TOP:320px;z-index:3;'><a href='https://bbs.huaweicloud.com/forum/thread-16526-1-1.html' target='_blank'><img src='./images/couplets/hw_cp_20190411_01.png'/></a></div>"
		//suspendcode14="<DIV id=\"lovexin14\" style='width:120px;height:270px;right:2px;POSITION:absolute;TOP:320px;z-index:3;'><a href='https://activity.huaweicloud.com/2019june_promotion/index.html?utm_source=huawei&utm_medium=other&utm_campaign=618dacu&utm_content=0614#app-connection' target='_blank'><img src='./images/couplets/hw_cp_20190612.png'/></a></div>"
		//document.write(suspendcode12); 
		//document.write(suspendcode14); 
		//window.setInterval("heartBeat()",1);
}

$(document).ready(function(){ 
	
});
</script>
